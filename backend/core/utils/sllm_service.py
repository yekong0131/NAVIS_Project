# backend/core/utils/sllm_service.py

import os
import json
import torch
import re
from core.utils.search_engine import SearchEngine
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from django.conf import settings


def dev_print(*args, **kwargs):
    if os.getenv("APP_ENV") == "development":
        print(*args, **kwargs)


# ==========================================
# 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜
# ==========================================
ADAPTER_PATH = os.path.join(settings.BASE_DIR, "core", "ai_models", "saved_adapter")

BASE_MODEL_PATH = "EleutherAI/polyglot-ko-1.3b"
# BASE_MODEL_PATH = "meta-llama/Llama-3.2-3B-Instruct"

JSON_DATA_PATH = os.path.join(settings.BASE_DIR, "data", "processed_clean_data.json")

llm_model = None
llm_tokenizer = None
search_engine = None

WATER_MAP = {}
EGI_MAP = {}

PROMPT_WATER_TRANSLATION = {"muddy": "íƒí•¨", "clear": "ë§‘ìŒ", "medium": "ë³´í†µ"}
PROMPT_EGI_TRANSLATION = {
    "blue": "íŒŒë‘",
    "brown": "ê°ˆìƒ‰",
    "green": "ì´ˆë¡",
    "orange": "ì£¼í™©",
    "pink": "í•‘í¬",
    "purple": "ë³´ë¼",
    "rainbow": "ë¬´ì§€ê°œ",
    "red": "ë¹¨ê°•",
    "yellow": "ë…¸ë‘",
}


# ==========================================
# 2. ë¡œë”© í•¨ìˆ˜ (Lazy Loading)
# ==========================================
def load_rag_data():
    global WATER_MAP, EGI_MAP
    if not os.path.exists(JSON_DATA_PATH):
        return
    try:
        with open(JSON_DATA_PATH, "r", encoding="utf-8") as f:
            json_dict = json.load(f)

        water_data = json_dict.get("í™˜ê²½", {}).get("ë¬¼ìƒ‰", {})
        for k, v in water_data.items():
            words = [k] + list(v.keys())
            for syns in v.values():
                words.extend(syns)
            WATER_MAP[k] = " ".join(list(set(words)))

        egi_data = json_dict.get("ì—ê¸°", {}).get("ì—ê¸° ìƒ‰ìƒ", {})
        for k, v in egi_data.items():
            words = [k] + list(v.keys())
            for syns in v.values():
                words.extend(syns)
            EGI_MAP[k] = " ".join(list(set(words)))
        dev_print("âœ… [RAG] Data Loaded.")
    except Exception as e:
        print(f"âŒ [RAG] Load Error: {e}")


def load_llm_model():
    """ì‹¤ì œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ëŠ” í•¨ìˆ˜ (Base + Adapter)"""
    global llm_model, llm_tokenizer, search_engine

    dev_print("â³ [Lazy Load] AI ëª¨ë¸ ë¡œë”© ì‹œì‘...")

    load_rag_data()

    try:
        search_engine = SearchEngine()
        dev_print("âœ… [Search] Connected.")
    except Exception as e:
        dev_print(f"âš ï¸ [Search] Connection Failed: {e}")
        search_engine = None

    if not torch.cuda.is_available():
        # [SERVER Case] GPUê°€ ì—†ëŠ” í™˜ê²½ (AWS t2.micro ë“±)
        print("\n" + "=" * 40)
        print("âš ï¸  [System] GPU ì‚¬ìš© ë¶ˆê°€ (CPU Mode).")
        print("ğŸ›‘  LLM ë¡œë”© ê±´ë„ˆëœ€")
        print("âœ…  ê¸°ë³¸ ì‘ë‹µ return")
        print("=" * 40 + "\n")

        # ëª¨ë¸ì„ Noneìœ¼ë¡œ ìœ ì§€ -> generate í•¨ìˆ˜ê°€ ì•Œì•„ì„œ ê¸°ë³¸ ë©˜íŠ¸ë¥¼ ë°˜í™˜í•¨
        llm_model = None
        return

    try:
        dev_print("â³ [Lazy Load] AI ëª¨ë¸ ë¡œë”© ì‹œì‘... (GPU Mode)")
        # 1. 4-bit ì–‘ìí™” ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # 2. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (Polyglot)
        dev_print(f"ğŸš€ Base Model ë¡œë“œ ì¤‘: {BASE_MODEL_PATH}")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
        )

        # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
        llm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)

        # 4. LoRA ì–´ëŒ‘í„° ì¥ì°© (í•µì‹¬!)
        dev_print(f"ğŸ”— Adapter ì¥ì°© ì¤‘: {ADAPTER_PATH}")
        llm_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
        llm_model.eval()

        dev_print("âœ… [LLM] Base Model + Adapter Loaded Successfully!")
    except Exception as e:
        print(f"âŒ [LLM] Load Failed: {e}")
        llm_model = None


# ==========================================
# 3. ê²€ìƒ‰ ë° ìƒì„±
# ==========================================
def get_relevant_context(water, egi):
    if not search_engine:
        return ""
    w_q = WATER_MAP.get(water, water)
    e_q = EGI_MAP.get(egi, egi)
    try:
        results = search_engine.search(f"{w_q} {e_q}", top_k=3)
        return " ".join(list(set(results))) if results else "ì •ë³´ ì—†ìŒ"
    except:
        return ""


def generate_recommendation_reason(water_color, egi_color, env_data):
    global llm_model, llm_tokenizer

    # 1. ë²ˆì—­ (ì˜ì–´ -> í•œê¸€ ë³€í™˜)
    p_water = PROMPT_WATER_TRANSLATION.get(water_color, water_color)
    p_egi = PROMPT_EGI_TRANSLATION.get(egi_color, egi_color)

    if llm_model is None:
        load_llm_model()

    if not llm_model:
        fallback_reason = f"í˜„ì¬ ê´€ì¸¡ëœ {p_water} ë¬¼ìƒ‰ í™˜ê²½ì—ì„œëŠ” ì‹œì¸ì„±ì´ ì¢‹ì€ {p_egi} ìƒ‰ìƒì˜ ì—ê¸°ê°€ ëŒ€ìƒì–´ì—ê²Œ ê°€ì¥ ê°•ë ¥í•˜ê²Œ ì–´í•„í•  ìˆ˜ ìˆì–´ ì¶”ì²œí•©ë‹ˆë‹¤."
        return fallback_reason, "Rule-based Fallback (No LLM)"

    try:
        # 1. ë¬¸ë§¥ ì¤€ë¹„
        context_text = get_relevant_context(water_color, egi_color)
        p_water = PROMPT_WATER_TRANSLATION.get(water_color, water_color)
        p_egi = PROMPT_EGI_TRANSLATION.get(egi_color, egi_color)

        prompt = (
            "ë‹¹ì‹ ì€ ë‚šì‹œì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ë¬¼ìƒ‰ê³¼ ì—ê¸°ìƒ‰ì— ëŒ€í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.\n"
            "ìŠ¤í¬ë¦½íŠ¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ë¬¼ìƒ‰ì— ì—ê¸°ìƒ‰ì„ ì¶”ì²œí•˜ëŠ” ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
            f"### ë¬¼ìƒ‰:{p_water}, ì—ê¸°ìƒ‰:{p_egi}\n"
            f"### ìŠ¤í¬ë¦½íŠ¸:\n{context_text}\n\n"
            "### ì¶”ì²œ ê·¼ê±°:\n"
        )

        # 2. í† í°í™”
        inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)

        if "token_type_ids" in inputs:
            del inputs["token_type_ids"]

        # 3. ìƒì„± (ë°˜ë³µ ë°©ì§€ ì„¤ì • ê°•í™”)
        with torch.no_grad():
            outputs = llm_model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.1,
                repetition_penalty=1.3,
                do_sample=True,
                eos_token_id=llm_tokenizer.eos_token_id,
                pad_token_id=llm_tokenizer.eos_token_id,
            )

        # 1. ì „ì²´ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        full_output = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

        # 2. "### ì¶”ì²œ ê·¼ê±°:" ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        if "### ì¶”ì²œ ê·¼ê±°:" in full_output:
            reason = full_output.split("### ì¶”ì²œ ê·¼ê±°:")[-1].strip()
        else:
            # ì‹¤íŒ¨ ì‹œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë§Œí¼ ìë¥´ê¸°
            input_len = inputs.input_ids.shape[1]
            generated_tokens = outputs[0][input_len:]
            reason = llm_tokenizer.decode(
                generated_tokens, skip_special_tokens=True
            ).strip()

        # 3. ë’·ë¶€ë¶„ ì°Œêº¼ê¸° ì œê±°
        stop_markers = ["ë‹¹ì‹ ì€ ë‚šì‹œì „ë¬¸ê°€ì…ë‹ˆë‹¤", "###", "ì°¸ê³ ë¡œ í˜„ì¬"]
        for marker in stop_markers:
            if marker in reason:
                reason = reason.split(marker)[0].strip()

        reason = reason.rstrip(",. ") + "."

        if len(reason) < 5 in reason:
            reason = f"{p_water} ë¬¼ìƒ‰ì—ëŠ” {p_egi} ìƒ‰ìƒì´ ê°€ì¥ ìœ ë¦¬í•˜ì—¬ ì¶”ì²œí•©ë‹ˆë‹¤."

        return reason, prompt

    except Exception as e:
        print(f"Gen Error: {e}")
        return f"{p_water} ë¬¼ìƒ‰ì—ëŠ” {p_egi} ìƒ‰ìƒì´ ìœ ë¦¬í•©ë‹ˆë‹¤.", str(e)
