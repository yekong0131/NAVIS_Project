# backend/core/utils/sllm_service.py

import os
import json
import torch
import re
from core.utils.search_engine import SearchEngine
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from django.conf import settings

# os.environ["CUDA_VISIBLE_DEVICES"] = "-1"


def dev_print(*args, **kwargs):
    if os.getenv("APP_ENV") == "development":
        print(*args, **kwargs)


# ==========================================
# 1. ì„¤ì • ë° ì „ì—­ ë³€ìˆ˜
# ==========================================
ADAPTER_PATH = os.path.join(settings.BASE_DIR, "core", "ai_models", "saved_adapter")

BASE_MODEL_PATH = "EleutherAI/polyglot-ko-1.3b"
# BASE_MODEL_PATH = "meta-llama/Llama-3.2-3B-Instruct"

JSON_DATA_PATH = os.path.join(settings.BASE_DIR, "data", "processed_clean_data.json")

llm_model = None
llm_tokenizer = None
search_engine = None

WATER_MAP = {}
EGI_MAP = {}

PROMPT_WATER_TRANSLATION = {"muddy": "íƒí•¨", "clear": "ë§‘ìŒ", "medium": "ë³´í†µ"}
PROMPT_EGI_TRANSLATION = {
    "blue": "íŒŒë‘",
    "brown": "ê°ˆìƒ‰",
    "green": "ì´ˆë¡",
    "orange": "ì£¼í™©",
    "pink": "í•‘í¬",
    "purple": "ë³´ë¼",
    "rainbow": "ë¬´ì§€ê°œ",
    "red": "ë¹¨ê°•",
    "yellow": "ë…¸ë‘",
}


# ==========================================
# 2. ë¡œë”© í•¨ìˆ˜ (Lazy Loading)
# ==========================================
def load_rag_data():
    global WATER_MAP, EGI_MAP
    if not os.path.exists(JSON_DATA_PATH):
        return
    try:
        with open(JSON_DATA_PATH, "r", encoding="utf-8") as f:
            json_dict = json.load(f)

        water_data = json_dict.get("í™˜ê²½", {}).get("ë¬¼ìƒ‰", {})
        for k, v in water_data.items():
            words = [k] + list(v.keys())
            for syns in v.values():
                words.extend(syns)
            WATER_MAP[k] = " ".join(list(set(words)))

        egi_data = json_dict.get("ì—ê¸°", {}).get("ì—ê¸° ìƒ‰ìƒ", {})
        for k, v in egi_data.items():
            words = [k] + list(v.keys())
            for syns in v.values():
                words.extend(syns)
            EGI_MAP[k] = " ".join(list(set(words)))
        dev_print("âœ… [RAG] Data Loaded.")
    except Exception as e:
        print(f"âŒ [RAG] Load Error: {e}")


def load_llm_model():
    """
    í™˜ê²½ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëª¨ë¸ì„ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜
    1. GPU(Local/High-Spec Server): 4bit ì–‘ìí™”ë¡œ ê³ ì† ë¡œë”©
    2. CPU(t3.medium): RAM/Swapì„ ì‚¬ìš©í•˜ì—¬ ë¡œë”© ì‹œë„ -> ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë©˜íŠ¸ ì‚¬ìš©
    """
    global llm_model, llm_tokenizer, search_engine

    dev_print("â³ [Lazy Load] AI ëª¨ë¸ ë¡œë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘...")

    load_rag_data()

    # ê²€ìƒ‰ ì—”ì§„ ì—°ê²°
    try:
        search_engine = SearchEngine()
        dev_print("âœ… [Search] Connected.")
    except Exception as e:
        dev_print(f"âš ï¸ [Search] Connection Failed: {e}")
        search_engine = None

    # ---------------------------------------------------------
    # CASE A: GPUê°€ ìˆëŠ” ê²½ìš° (ê°œë°œ í™˜ê²½)
    # ---------------------------------------------------------
    if torch.cuda.is_available():
        try:
            dev_print("ğŸš€ GPU Detected! Loading with 4-bit Quantization...")

            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )

            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL_PATH,
                quantization_config=bnb_config,
                device_map="auto",
            )

            llm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)

            dev_print(f"ğŸ”— Adapter ì¥ì°© ì¤‘ (GPU): {ADAPTER_PATH}")
            llm_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
            llm_model.eval()

            dev_print("âœ… [LLM] GPU Mode Loaded Successfully!")
            return

        except Exception as e:
            print(f"âŒ [GPU Load Error] {e}")
            llm_model = None
            return

    # ---------------------------------------------------------
    # CASE B: GPUê°€ ì—†ëŠ” ê²½ìš° (AWS t3.medium)
    # ---------------------------------------------------------
    else:
        print("âš ï¸ [System] No GPU detected. Attempting CPU Load...")
        print("â³ t3.medium ë©”ëª¨ë¦¬ í•œê³„ í…ŒìŠ¤íŠ¸ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤)")

        try:
            base_model = AutoModelForCausalLM.from_pretrained(
                BASE_MODEL_PATH,
                dtype=torch.float32,
                device_map="cpu",
                low_cpu_mem_usage=True,
            )

            llm_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)

            dev_print(f"ğŸ”— Adapter ì¥ì°© ì¤‘ (CPU): {ADAPTER_PATH}")
            llm_model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
            llm_model.eval()

            dev_print("âœ… [LLM] CPU Mode Loaded! (ì†ë„ëŠ” ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

        except (RuntimeError, MemoryError) as e:
            dev_print("\n" + "=" * 50)
            dev_print(f"âŒ [Memory Error] ì„œë²„ ìš©ëŸ‰ ë¶€ì¡±ìœ¼ë¡œ LLM ë¡œë”© ì‹¤íŒ¨.")
            dev_print(f"ğŸ’¬ Error Detail: {e}")
            dev_print("âœ… 'ê¸°ë³¸ ë©˜íŠ¸(Rule-based)' ëª¨ë“œë¡œ ìë™ ì „í™˜í•©ë‹ˆë‹¤.")
            dev_print("=" * 50 + "\n")
            llm_model = None

        except Exception as e:
            print(f"âŒ [Unknown Error] CPU ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            llm_model = None

        # print("\n" + "=" * 40)
        # print("âš ï¸  [System] Server Mode (No GPU).")
        # print("ğŸ›‘  Skipping LLM Load for Performance.")
        # print("âœ…  'Rule-based Fallback' ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.")
        # print("=" * 40 + "\n")

        # # ëª¨ë¸ì„ Noneìœ¼ë¡œ ë‘ë©´, ë‚˜ì¤‘ì— generate í•¨ìˆ˜ê°€ ì•Œì•„ì„œ ê¸°ë³¸ ë©˜íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.
        # llm_model = None
        # return


# ==========================================
# 3. ê²€ìƒ‰ ë° ìƒì„±
# ==========================================
def get_relevant_context(water, egi):
    if not search_engine:
        return ""
    w_q = WATER_MAP.get(water, water)
    e_q = EGI_MAP.get(egi, egi)
    try:
        results = search_engine.search(f"{w_q} {e_q}", top_k=3)
        return " ".join(list(set(results))) if results else "ì •ë³´ ì—†ìŒ"
    except:
        return ""


def generate_recommendation_reason(water_color, egi_color, env_data):
    global llm_model, llm_tokenizer

    # 1. ë²ˆì—­ (ì˜ì–´ -> í•œê¸€ ë³€í™˜)
    p_water = PROMPT_WATER_TRANSLATION.get(water_color, water_color)
    p_egi = PROMPT_EGI_TRANSLATION.get(egi_color, egi_color)

    if llm_model is None:
        load_llm_model()

    if not llm_model:
        fallback_reason = f"í˜„ì¬ ê´€ì¸¡ëœ {p_water} ë¬¼ìƒ‰ í™˜ê²½ì—ì„œëŠ” ì‹œì¸ì„±ì´ ì¢‹ì€ {p_egi} ìƒ‰ìƒì˜ ì—ê¸°ê°€ ëŒ€ìƒì–´ì—ê²Œ ê°€ì¥ ê°•ë ¥í•˜ê²Œ ì–´í•„í•  ìˆ˜ ìˆì–´ ì¶”ì²œí•©ë‹ˆë‹¤."
        return fallback_reason, "Rule-based Fallback (No LLM)"

    try:
        # 1. ë¬¸ë§¥ ì¤€ë¹„
        context_text = get_relevant_context(water_color, egi_color)
        p_water = PROMPT_WATER_TRANSLATION.get(water_color, water_color)
        p_egi = PROMPT_EGI_TRANSLATION.get(egi_color, egi_color)

        prompt = (
            "ë‹¹ì‹ ì€ ë‚šì‹œì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ë¬¼ìƒ‰ê³¼ ì—ê¸°ìƒ‰ì— ëŒ€í•œ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.\n"
            "ìŠ¤í¬ë¦½íŠ¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ë¬¼ìƒ‰ì— ì—ê¸°ìƒ‰ì„ ì¶”ì²œí•˜ëŠ” ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
            f"### ë¬¼ìƒ‰:{p_water}, ì—ê¸°ìƒ‰:{p_egi}\n"
            f"### ìŠ¤í¬ë¦½íŠ¸:\n{context_text}\n\n"
            "### ì¶”ì²œ ê·¼ê±°:\n"
        )

        # 2. í† í°í™”
        inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)

        if "token_type_ids" in inputs:
            del inputs["token_type_ids"]

        # 3. ìƒì„± (ë°˜ë³µ ë°©ì§€ ì„¤ì • ê°•í™”)
        with torch.no_grad():
            outputs = llm_model.generate(
                **inputs,
                max_new_tokens=150,
                temperature=0.1,
                repetition_penalty=1.3,
                do_sample=True,
                eos_token_id=llm_tokenizer.eos_token_id,
                pad_token_id=llm_tokenizer.eos_token_id,
            )

        # 1. ì „ì²´ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        full_output = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

        # 2. "### ì¶”ì²œ ê·¼ê±°:" ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
        if "### ì¶”ì²œ ê·¼ê±°:" in full_output:
            reason = full_output.split("### ì¶”ì²œ ê·¼ê±°:")[-1].strip()
        else:
            # ì‹¤íŒ¨ ì‹œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë§Œí¼ ìë¥´ê¸°
            input_len = inputs.input_ids.shape[1]
            generated_tokens = outputs[0][input_len:]
            reason = llm_tokenizer.decode(
                generated_tokens, skip_special_tokens=True
            ).strip()

        # 3. ë’·ë¶€ë¶„ ì°Œêº¼ê¸° ì œê±°
        stop_markers = ["ë‹¹ì‹ ì€ ë‚šì‹œì „ë¬¸ê°€ì…ë‹ˆë‹¤", "###", "ì°¸ê³ ë¡œ í˜„ì¬"]
        for marker in stop_markers:
            if marker in reason:
                reason = reason.split(marker)[0].strip()

        reason = reason.rstrip(",. ") + "."

        if len(reason) < 5 in reason:
            reason = f"{p_water} ë¬¼ìƒ‰ì—ëŠ” {p_egi} ìƒ‰ìƒì´ ê°€ì¥ ìœ ë¦¬í•˜ì—¬ ì¶”ì²œí•©ë‹ˆë‹¤."

        return reason, prompt

    except Exception as e:
        print(f"Gen Error: {e}")
        return f"{p_water} ë¬¼ìƒ‰ì—ëŠ” {p_egi} ìƒ‰ìƒì´ ìœ ë¦¬í•©ë‹ˆë‹¤.", str(e)
