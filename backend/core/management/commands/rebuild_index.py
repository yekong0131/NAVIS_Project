# backend/core/management/commands/rebuild_index.py

import os
import json
import re
from django.conf import settings
from django.core.management.base import BaseCommand
from core.utils.search_engine import SearchEngine
from kiwipiepy import Kiwi  # pip install kiwipiepy í•„ìš”


class Command(BaseCommand):
    help = "JSON ë°ì´í„°ì™€ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì„ ì½ì–´ Elasticsearch ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•©ë‹ˆë‹¤."

    def handle(self, *args, **options):
        self.stdout.write("ğŸš€ ê²€ìƒ‰ ì—”ì§„ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # 1. ì—”ì§„ ë° Kiwi ì´ˆê¸°í™”
        engine = SearchEngine(index_name="fishing_scripts")
        engine.create_index()  # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„±
        kiwi = Kiwi()

        # 2. JSON ë°ì´í„° ë¡œë“œ
        json_path = os.path.join(settings.BASE_DIR, "data", "processed_clean_data.json")

        if not os.path.exists(json_path):
            self.stdout.write(
                self.style.ERROR(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
            )
            return

        with open(json_path, "r", encoding="utf-8") as f:
            json_dict = json.load(f)

        # ---------------------------------------------------------
        # [ë¡œì§ 1] Water Map & Egi Map ìƒì„±
        # ---------------------------------------------------------
        water_map = {}
        water_data = json_dict.get("í™˜ê²½", {}).get("ë¬¼ìƒ‰", {})
        for condition, details in water_data.items():
            keywords = [condition]
            for sub_key, synonyms in details.items():
                keywords.append(sub_key)
                keywords.extend(synonyms)
            water_map[condition] = list(set(keywords))

        # ---------------------------------------------------------
        # [ë¡œì§ 2] ì˜¤íƒ€ ë³´ì • ë§µ ìƒì„±
        # ---------------------------------------------------------
        core_keywords = set()
        correction_map = {}

        def extract_typos(data):
            if isinstance(data, dict):
                for k, v in data.items():
                    if isinstance(v, list):
                        core_keywords.add(k)
                        for typo in v:
                            correction_map[typo] = k
                            core_keywords.add(typo)
                    else:
                        extract_typos(v)

        extract_typos(json_dict)

        # 3. ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„° ìƒ‰ì¸ (Kiwi ì‚¬ìš©)
        script_folder = os.path.join(settings.BASE_DIR, "scripts")  # ê²½ë¡œ í™•ì¸ í•„ìš”

        # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        if not os.path.exists(script_folder):
            os.makedirs(script_folder)
            self.stdout.write(f"ğŸ“‚ ìŠ¤í¬ë¦½íŠ¸ í´ë”ê°€ ì—†ì–´ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {script_folder}")
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            with open(
                os.path.join(script_folder, "test_sample.txt"), "w", encoding="utf-8"
            ) as f:
                f.write(
                    "ë¬¼ì´ íƒí•  ë•ŒëŠ” ê³ ì¶”ì¥ ì—ê¸°ê°€ ì¢‹ìŠµë‹ˆë‹¤. ë°˜ë©´ ë¬¼ì´ ë§‘ìœ¼ë©´ ë„¤ì¸„ëŸ´ ì»¬ëŸ¬ë¥¼ ì“°ì„¸ìš”."
                )

        file_names = os.listdir(script_folder)
        doc_id = 0
        total_sentences = 0

        for file_name in file_names:
            if not file_name.endswith(".txt"):
                continue

            file_path = os.path.join(script_folder, file_name)
            with open(file_path, "r", encoding="utf-8") as f:
                raw_text = f.read()

            # í…ìŠ¤íŠ¸ ì •ì œ
            clean_text = re.sub(r"\|\d+:\d+", "", raw_text)  # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
            clean_text = re.sub(r"\.|\n", " ", clean_text)
            clean_text = re.sub(r"[ ]+", " ", clean_text)

            # [í•µì‹¬] Kiwië¡œ ë¬¸ì¥ ë¶„ë¦¬
            sentences = kiwi.split_into_sents(clean_text)
            sent_list = [s.text.strip() for s in sentences]

            for i, line in enumerate(sent_list):
                # ë¬¸ë§¥(Context) í™•ë³´: ì•ë’¤ ë¬¸ì¥ í¬í•¨
                start_idx = max(0, i - 1)
                end_idx = min(len(sent_list), i + 2)
                context_line = " ".join(sent_list[start_idx:end_idx])

                # ì¡ë‹´ í•„í„°ë§
                if any(
                    k in line for k in ["êµ¬ë…", "ì¢‹ì•„ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì•ˆë…•í•˜ì„¸ìš”"]
                ):
                    continue

                # ì˜¤íƒ€ ë³´ì •
                fixed_line = context_line
                for typo, correct in correction_map.items():
                    fixed_line = fixed_line.replace(typo, correct)

                # ê²€ìƒ‰ì–´ ì¶”ì¶œ (Okt ì‚¬ìš© - SearchEngine ë‚´ë¶€ ë©”ì„œë“œ)
                index_terms = engine.tokenize(fixed_line)

                # ë©”íƒ€ë°ì´í„° íƒœê¹… (ë¬¼ìƒ‰ ë“±)
                water_type = "medium"
                for w_key, w_keywords in water_map.items():
                    if any(k in fixed_line for k in w_keywords):
                        water_type = w_key
                        break

                meta = {"water": water_type, "source": file_name}

                # Elasticsearchì— ì €ì¥
                engine.insert_script(doc_id, i, index_terms, fixed_line, meta)
                total_sentences += 1

            doc_id += 1
            self.stdout.write(f"   -> {file_name} ìƒ‰ì¸ ì™„ë£Œ")

        self.stdout.write(
            self.style.SUCCESS(
                f"âœ… ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ! ì´ {total_sentences}ê°œì˜ ë¬¸ì¥ì´ ìƒ‰ì¸ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
        )
